/**
 * –°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –ë–∏—Ç—Ä–∏–∫—Å –≤ S3
 * 
 * - –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ —Å—Ç–∞—Ä–æ–≥–æ —Å–∞–π—Ç–∞
 * - –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ WebP
 * - –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤ S3
 * - –û–±–Ω–æ–≤–ª—è–µ—Ç URL –≤ —Ç–æ–≤–∞—Ä–∞—Ö Medusa
 * 
 * –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
 * npx medusa exec ./src/scripts/migrate-images.ts
 */

import { ExecArgs } from "@medusajs/framework/types"
import { Modules, ContainerRegistrationKeys } from "@medusajs/framework/utils"
import { S3Client, PutObjectCommand } from "@aws-sdk/client-s3"
import sharp from "sharp"
import * as fs from "fs"
import * as path from "path"

// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
const OLD_SITE_URL = "https://24-karat.ru"
const S3_BUCKET = process.env.S3_BUCKET || "e52e01db-34753286-a744-4ecf-acd1-5303dbd3c54f"
const S3_FILE_URL = process.env.S3_FILE_URL || "https://s3.twcstorage.ru/" + S3_BUCKET

// S3 –∫–ª–∏–µ–Ω—Ç
const s3Client = new S3Client({
  region: process.env.S3_REGION || "ru-1",
  endpoint: process.env.S3_ENDPOINT || "https://s3.twcstorage.ru",
  credentials: {
    accessKeyId: process.env.S3_ACCESS_KEY_ID!,
    secretAccessKey: process.env.S3_SECRET_ACCESS_KEY!,
  },
  forcePathStyle: true,
})

// –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —ç–∫—Å–ø–æ—Ä—Ç–æ–º
const EXPORT_DIR = path.join(process.cwd(), "bitrix-export")

interface BitrixFile {
  id: number
  path: string
  original_name: string
  content_type: string
}

// –ó–∞–≥—Ä—É–∑–∫–∞ JSON
function loadJson<T>(filename: string): T[] {
  const filepath = path.join(EXPORT_DIR, filename)
  if (!fs.existsSync(filepath)) {
    return []
  }
  return JSON.parse(fs.readFileSync(filepath, "utf-8"))
}

// –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
async function downloadFile(url: string): Promise<Buffer | null> {
  try {
    const response = await fetch(url, {
      headers: {
        "User-Agent": "Mozilla/5.0 (compatible; MigrationBot/1.0)",
      },
    })
    
    if (!response.ok) {
      return null
    }
    
    return Buffer.from(await response.arrayBuffer())
  } catch (error) {
    return null
  }
}

// –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WebP
async function convertToWebP(buffer: Buffer): Promise<Buffer> {
  return sharp(buffer)
    .webp({ quality: 85 })
    .toBuffer()
}

// –ó–∞–≥—Ä—É–∑–∫–∞ –≤ S3
async function uploadToS3(buffer: Buffer, key: string, contentType: string): Promise<string> {
  await s3Client.send(new PutObjectCommand({
    Bucket: S3_BUCKET,
    Key: key,
    Body: buffer,
    ContentType: contentType,
  }))
  
  return `${S3_FILE_URL}/${key}`
}

// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –¥–ª—è S3
function generateS3Key(originalPath: string, isWebP: boolean): string {
  const basename = path.basename(originalPath, path.extname(originalPath))
  const ext = isWebP ? ".webp" : path.extname(originalPath)
  const timestamp = Date.now()
  return `products/${timestamp}-${basename}${ext}`
}

export default async function migrateImages({ container }: ExecArgs) {
  const logger = container.resolve(ContainerRegistrationKeys.LOGGER)
  
  logger.info("üñºÔ∏è –ù–∞—á–∞–ª–æ –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
  
  // –ü—Ä–æ–≤–µ—Ä—è–µ–º S3 –∫—Ä–µ–¥—ã
  if (!process.env.S3_ACCESS_KEY_ID || !process.env.S3_SECRET_ACCESS_KEY) {
    logger.error("‚ùå S3 credentials not configured!")
    return
  }
  
  // –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Ä—Ç—É —Ñ–∞–π–ª–æ–≤
  const files = loadJson<BitrixFile>("files_map.json")
  
  if (files.length === 0) {
    logger.error("‚ùå files_map.json –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç")
    return
  }
  
  logger.info(`üìÅ –ù–∞–π–¥–µ–Ω–æ ${files.length} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏`)
  
  // –ü–æ–ª—É—á–∞–µ–º —Å–µ—Ä–≤–∏—Å –ø—Ä–æ–¥—É–∫—Ç–æ–≤
  const productService = container.resolve(Modules.PRODUCT)
  
  // –°–æ–∑–¥–∞—ë–º –∫–∞—Ä—Ç—É —Å—Ç–∞—Ä—ã—Ö URL -> –Ω–æ–≤—ã—Ö URL
  const urlMap = new Map<string, string>()
  
  // –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
  const stats = {
    downloaded: 0,
    converted: 0,
    uploaded: 0,
    skipped: 0,
    errors: 0,
  }
  
  // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –ø–∞—á–∫–∞–º–∏
  const BATCH_SIZE = 10
  
  for (let i = 0; i < files.length; i += BATCH_SIZE) {
    const batch = files.slice(i, i + BATCH_SIZE)
    
    await Promise.all(batch.map(async (file) => {
      const oldUrl = `${OLD_SITE_URL}${file.path}`
      
      try {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ª–∏ —É–∂–µ
        if (urlMap.has(oldUrl)) {
          stats.skipped++
          return
        }
        
        // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if (!file.content_type?.startsWith("image/")) {
          stats.skipped++
          return
        }
        
        // –°–∫–∞—á–∏–≤–∞–µ–º
        const buffer = await downloadFile(oldUrl)
        if (!buffer) {
          logger.warn(`  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å: ${file.path}`)
          stats.errors++
          return
        }
        stats.downloaded++
        
        // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WebP (–µ—Å–ª–∏ –Ω–µ GIF –∏ –Ω–µ SVG)
        let processedBuffer = buffer
        let isWebP = false
        
        if (!file.content_type.includes("gif") && !file.content_type.includes("svg")) {
          processedBuffer = await convertToWebP(buffer)
          isWebP = true
          stats.converted++
        }
        
        // –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ S3
        const s3Key = generateS3Key(file.path, isWebP)
        const newUrl = await uploadToS3(
          processedBuffer, 
          s3Key, 
          isWebP ? "image/webp" : file.content_type
        )
        stats.uploaded++
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
        urlMap.set(oldUrl, newUrl)
        
        // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        const compressionRatio = isWebP 
          ? Math.round((1 - processedBuffer.length / buffer.length) * 100)
          : 0
        
        logger.info(`  ‚úÖ ${file.original_name} -> ${s3Key} ${isWebP ? `(-${compressionRatio}%)` : ""}`)
        
      } catch (error) {
        stats.errors++
        logger.error(`  ‚ùå –û—à–∏–±–∫–∞: ${file.path} - ${error}`)
      }
    }))
    
    // –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
    await new Promise(resolve => setTimeout(resolve, 500))
    
    // –ü—Ä–æ–≥—Ä–µ—Å—Å
    const progress = Math.min(i + BATCH_SIZE, files.length)
    logger.info(`üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: ${progress}/${files.length} (${Math.round(progress / files.length * 100)}%)`)
  }
  
  // –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ URL –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
  const mappingPath = path.join(EXPORT_DIR, "url_mapping.json")
  fs.writeFileSync(
    mappingPath, 
    JSON.stringify(Object.fromEntries(urlMap), null, 2)
  )
  logger.info(`üíæ –ú–∞–ø–ø–∏–Ω–≥ URL —Å–æ—Ö—Ä–∞–Ω—ë–Ω: ${mappingPath}`)
  
  // –û–±–Ω–æ–≤–ª—è–µ–º URL –≤ —Ç–æ–≤–∞—Ä–∞—Ö Medusa
  logger.info("\nüîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ URL –≤ —Ç–æ–≤–∞—Ä–∞—Ö...")
  
  const products = await productService.listProducts({}, { relations: ["images"] })
  let updatedProducts = 0
  
  for (const product of products) {
    if (!product.images || product.images.length === 0) continue
    
    const updatedImages = product.images.map(img => {
      const newUrl = urlMap.get(img.url)
      return newUrl ? { url: newUrl } : { url: img.url }
    })
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    const hasChanges = product.images.some((img, idx) => 
      img.url !== updatedImages[idx].url
    )
    
    if (hasChanges) {
      await productService.updateProducts([{
        id: product.id,
        images: updatedImages,
      }])
      updatedProducts++
    }
  }
  
  logger.info(`  ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: ${updatedProducts}`)
  
  // –ò—Ç–æ–≥–∏
  logger.info("\n" + "=".repeat(50))
  logger.info("üìä –ò–¢–û–ì–ò –ú–ò–ì–†–ê–¶–ò–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:")
  logger.info(`   –°–∫–∞—á–∞–Ω–æ: ${stats.downloaded}`)
  logger.info(`   –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ WebP: ${stats.converted}`)
  logger.info(`   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤ S3: ${stats.uploaded}`)
  logger.info(`   –ü—Ä–æ–ø—É—â–µ–Ω–æ: ${stats.skipped}`)
  logger.info(`   –û—à–∏–±–æ–∫: ${stats.errors}`)
  logger.info(`   –¢–æ–≤–∞—Ä–æ–≤ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: ${updatedProducts}`)
  logger.info("=".repeat(50))
}

